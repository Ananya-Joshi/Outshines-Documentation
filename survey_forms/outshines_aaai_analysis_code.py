# -*- coding: utf-8 -*-
"""OutsHiNes AAAI Analysis Code

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1gjwzbQnBWIKHci2pk_ho_uobkuE4PrfC
"""

#Access Expert Responses
df = pd.read_csv("responses_and_explanations.csv")
df = pd.DataFrame(rows[1:], columns=rows[0])
sel_df = df.iloc[:, 1:4]

#Process results as per preregistraion
acceptable_ind = sel_df.index[(sel_df=='Yes').sum(axis=1)==3]
df = df.loc[acceptable_ind, :]
bad_index = pd.concat([pd.Series(df.index[df.iloc[:, 4] != 'A']),pd.Series(df.index[df.iloc[:, 15] != 'B'])])
df = df.drop(index=bad_index)

#Response Consistency Metrics
import numpy as np
from sklearn.metrics import euclidean_distances
results = df.iloc[:, [5, 6, 7, 8, 9, 16, 17, 18, 19, 20]].applymap(lambda x: x.replace('(', '').replace(')', '').split(' ')[0]).replace('N/A', np.nan).astype(float) #
#this is a relative metric
consistency_user = results.apply(lambda x: rk.concordance(pd.DataFrame([x.fillna(0).values[[3,4]], x.fillna(0).values[[9,5]]]), axis=0), axis=1)
#this is an absolute metric
user_diff_rank = results.apply(lambda x: euclidean_distances([x[[3,4]].fillna(0).values] , [x[[9,5]].fillna(0).values]), axis=1)

#Compare different ranking mechanisms

alg_df_sib = pd.read_csv('sib_10.csv', index_col=0).iloc[:, 3:]
alg_df_outshines = pd.read_csv('outshines_10.csv', index_col=0).iloc[:, 3:]
alg_df_thresh =pd.read_csv('thresh_10.csv', index_col=0).iloc[:, 3:]
alg_df_thresh_opt =pd.read_csv('thresh_opt_10.csv', index_col=0).iloc[:, 3:]

from sklearn.metrics import accuracy_score, f1_score, roc_auc_score, balanced_accuracy_score
output_values_df = []
for i, (score_meth, comp_df) in enumerate({ 'Thresh':alg_df_thresh, 'Thresh Opt':alg_df_thresh_opt, 'Sibling':alg_df_sib,'Outshines':alg_df_outshines}.items()): #',
  for user, user_df in results.iterrows():
    user_a = user_df.iloc[:5]
    user_b = user_df.iloc[5:]
    binary_a = ~user_a.isna()
    binary_b = ~user_b.isna()
    user_a=user_a.fillna(0).values
    user_b=user_b.fillna(0).values
    for alg, alg_df in comp_df.T.iterrows():
      comp_a = alg_df.iloc[:5]
      comp_b = alg_df.iloc[5:]
      if i==0:#0:
        binary_comp_a = comp_a
        binary_comp_b = comp_b
      else:
        comp_a = (rk.rank(comp_a.fillna(0), reverse=True)*~comp_a.isna()).replace(0, np.nan)
        comp_b = (rk.rank(comp_b.fillna(0), reverse=True)*~comp_b.isna()).replace(0, np.nan)
        binary_comp_a = comp_a.index.isin(comp_a.nlargest(binary_a.sum()).index)
        binary_comp_b = comp_b.index.isin(comp_b.nlargest(binary_b.sum()).index)
      for name_met, metric in { 'Accuracy':accuracy_score, 'F1':f1_score, 'bal_acc':balanced_accuracy_score}.items():
        output_values_df.append({'score_meth':score_meth,'user':user, 'alg':alg, 'met':name_met, 'form':'a', 'val':metric(binary_a, binary_comp_a)})
        output_values_df.append({'score_meth':score_meth,'user':user, 'alg':alg, 'met':name_met, 'form':'b', 'val':metric(binary_b, binary_comp_b)})
      #ROC AUC Separate and only if multiclass:
      if len(pd.unique(binary_a))==2:
        output_values_df.append({'score_meth':score_meth,'user':user, 'alg':alg, 'met':'ROCAUC', 'form':'a', 'val':roc_auc_score(binary_a, comp_a)})
      if len(pd.unique(binary_b))==2:
        output_values_df.append({'score_meth':score_meth,'user':user, 'alg':alg, 'met':'ROCAUC', 'form':'b', 'val':roc_auc_score(binary_b, comp_b)})
      if i!=-1:
        # for name_met, metric in {'corr':, 'dist':}.items():
          output_values_df.append({'score_meth':score_meth,'user':user, 'alg':alg, 'met':'spear', 'form':'a', 'val':rk.corr(user_a, comp_a, method='spearman')})
          output_values_df.append({'score_meth':score_meth,'user':user, 'alg':alg, 'met':'spear', 'form':'b', 'val':rk.corr(user_b, comp_b, method='spearman')})
          output_values_df.append({'score_meth':score_meth,'user':user, 'alg':alg, 'met':'Correlation', 'form':'a', 'val':rk.corr(user_a, comp_a, method='swap')})
          output_values_df.append({'score_meth':score_meth,'user':user, 'alg':alg, 'met':'Correlation', 'form':'b', 'val':rk.corr(user_b, comp_b, method='swap')})
          output_values_df.append({'score_meth':score_meth,'user':user, 'alg':alg, 'met':'pear', 'form':'a', 'val':rk.corr(user_a, comp_a, method='pearson')})
          output_values_df.append({'score_meth':score_meth,'user':user, 'alg':alg, 'met':'pear', 'form':'b', 'val':rk.corr(user_b, comp_b, method='pearson')})
          output_values_df.append({'score_meth':score_meth,'user':user, 'alg':alg, 'met':'Distance', 'form':'a', 'val':rk.dist(user_a, comp_a, method='euclidean')})
          output_values_df.append({'score_meth':score_meth,'user':user, 'alg':alg, 'met':'Distance', 'form':'b', 'val':rk.dist(user_b, comp_b, method='euclidean')})
          output_values_df.append({'score_meth':score_meth,'user':user, 'alg':alg, 'met':'kt', 'form':'a', 'val':rk.kendall_tau_distance(user_a, comp_a)})
          output_values_df.append({'score_meth':score_meth,'user':user, 'alg':alg, 'met':'kt', 'form':'b', 'val':rk.kendall_tau_distance(user_b, comp_b)})
opmetrics = pd.DataFrame(output_values_df)
opmetrics = opmetrics.replace({'Distance':'Distance (lower is better)'})

#Plotting Results

import plotly.graph_objects as go
from plotly.subplots import make_subplots
import math

color_dict = {'Thresh':'navy', 'Thresh Opt':'mediumblue', 'Sibling':'cornflowerblue', 'Outshines':'deepskyblue'}
marker_dict = {'Thresh':".", 'Thresh Opt':"+", 'Sibling':"x", 'Outshines':""}
metric = ['Accuracy', 'F1', 'ROCAUC']
metric2 = ['Distance (lower is better)', 'Correlation']

scoring = ['Thresh', 'Thresh Opt', 'Sibling', 'Outshines']
fig = make_subplots(rows=1, cols=3, subplot_titles=metric, shared_xaxes=True, y_title='', x_title='')
for i, met in enumerate(metric):
  for j, score in enumerate(scoring):
    sub_df = opmetrics.query("met==@met and score_meth==@score")
    sub_df.alg = pd.Categorical(sub_df.alg,
                      categories=["EWMA", "FlaSH", "AR", "IF", "DL", "Tele"],
                      ordered=True)
    sub_df = sub_df.sort_values(by='alg')
    sub_mean = sub_df.groupby('alg').mean().val
    sub_CI = 1.96/sub_df.groupby('alg').size().apply(math.sqrt)*sub_df.groupby('alg').std().val
    fig.add_trace(go.Bar(x=sub_mean.index, y=sub_mean, name=score,   #marker_pattern_shape=marker_dict[score],
                             marker_color=color_dict[score], showlegend=(i==0),
                            legendgroup=score,
                            error_y=dict(
                                type='data',
                                symmetric=False,
                                array=sub_CI,
                                color='black',
                                arrayminus=sub_CI)
                            ), row=1, col=i+1)

fig.update_layout(margin=dict(l=20, r=20, t=20, b=20), height=260, width=1300, font=dict(
            family="Times New Roman",
            color='black'), legend=dict(
    orientation="h",
    yanchor="bottom",
    y=1.10,
    xanchor="left",
    x=0
), paper_bgcolor='rgba(0,0,0,0)',
    plot_bgcolor='rgba(0,0,0,0)', font_size=18)
fig.update_xaxes(showgrid=False, zeroline=True, linecolor='black')
fig.update_yaxes(showgrid=False, zeroline=True, linecolor='black')
fig.update_annotations(font_size=24)
fig.show(renderer='colab')
fig.write_image('metrics_binary.pdf', format="pdf")
fig = make_subplots(rows=1, cols=2, subplot_titles=metric2, shared_xaxes=True, y_title='', x_title='')
for i, met in enumerate(metric2):
  for j, score in enumerate(scoring):
    sub_df = opmetrics.query("met==@met and score_meth==@score")
    sub_df.alg = pd.Categorical(sub_df.alg,
                      categories=["EWMA", "FlaSH", "AR", "IF", "DL", "Tele"],
                      ordered=True)
    sub_df = sub_df.sort_values(by='alg')
    sub_mean = sub_df.groupby('alg').mean().val
    sub_CI = 1.96/sub_df.groupby('alg').size().apply(math.sqrt)*sub_df.groupby('alg').std().val
    fig.add_trace(go.Bar(x=sub_mean.index, y=sub_mean, name=score,   #marker_pattern_shape=marker_dict[score],
                           marker_color=color_dict[score], showlegend=False,
                            legendgroup=score,
                            error_y=dict(
                                type='data',
                                symmetric=False,
                                array=sub_CI,
                                color='black',
                                arrayminus=sub_CI)
                            ), row=1, col=i+1)
fig.update_layout(margin=dict(l=20, r=20, t=30, b=20), height=200, width=850, legend=dict(
    orientation="h",
    yanchor="bottom",
    y=1.10,
    xanchor="left",
    x=0
), font=dict(
            family="Times New Roman",
            color='black'),paper_bgcolor='rgba(0,0,0,0)',
    plot_bgcolor='rgba(0,0,0,0)', font_size=20)
fig.update_annotations(font_size=24)
fig.update_xaxes(showgrid=False, zeroline=True, linecolor='black')
fig.update_yaxes(showgrid=False, zeroline=True, linecolor='black')

fig.show(renderer='colab')

fig.write_image('metrics_ranking.pdf', format="pdf")